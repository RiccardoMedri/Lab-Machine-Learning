{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fondamenti di PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effettuiamo l'import delle librerie utilizzate nell'esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import ConfusionMatrixDisplay \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import auc, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito i riferimenti alle pagine di documentazione, sempre utiliti:\n",
    "\n",
    "* Rif: [numpy](https://numpy.org/doc/)\n",
    "* Rif: [pandas](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "* Rif: [matplotlib](https://matplotlib.org/stable/index.html)\n",
    "* Rif: [scikit](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Controllare la causalita'._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, prima di procedere all'utilizzo di dati 'casuali' per l'esercitazione, impostiamo un seed per la libreria _numpy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da questo momento in poi, ogni esecuzione dello script dara' sempre gli stessi risultati...a meno di cambiare nuovamente il seed o rimuovere la riga di codice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Definiamo N classi, etichette, da associare ai dati._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per procedere al calcolo di metriche, abbiamo bisogno di simulare dati casuali.\n",
    "Per le metriche che saranno calcolate, questi dati dovranno essere associati a delle etichette, o classi, o labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['circle', 'square', 'triangle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Creaiamo dei dati casuali._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immaginiamo di avere dei dati reali su cui un modello di deep learning deve fare inferenza.\n",
    "Immaginiamo di avere gi√† eseguito l'inferenza su questi dati e di avere ottenuto i risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = np.random.randint(0, len(classes), size=(num_data))\n",
    "pred_labels = np.random.randint(0, len(classes), size=(num_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dei dati avremo quindi:\n",
    "* Le previsioni **reali** attese.\n",
    "* Le previsioni, realmente, **predette**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _La matrice di confusione._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la matrice di confusione e' possibile mettere a confronto tutte le predizioni fatte da un modello, di classificazione, rispetto alle previsioni reali/attese. _Scikit_ possiede un metodo apposito per il calcolo e per l'utilizzo e' necessario solamente fornire le etichette reali e le etichette predette.\n",
    "\n",
    "* Rif: [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(real_labels, pred_labels)\n",
    "print('Confusion matrix:')\n",
    "print(f'|__Type:{type(cm)}')\n",
    "print(f'\\n{cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice e' a tutti gli effetti un array _numpy_. Si puo' visualizzare con una print e se ne possono utilizzare i metodi classici.\n",
    "Oltre questo, _scikit_ da' la possibilita' di visualizzarla in stile grafico.\n",
    "\n",
    "* Rif: [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _L'accuratezza della matrice._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Della matrice di confusione ci si puo' chiedere..._Quanto accurate sono state le previsioni complessive?_. Nella diagonale si trovano tutte le previsioni fatte dalla rete e che coincidono con il valore atteso. Sommare la diagonale e confrontarla con la totalita' delle previsioni e' il modo di calcolare l'accuratezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(real_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In automatico, _scikit_ puo' fornire lo stesso risultato con il metodo _accuracy_score_. Come in precedenza, sono richieste solamente le previsioni reali e predette.\n",
    "\n",
    "* Rif: [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Un report globale._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moltissime altre sono le metriche che _scikit_ permette di calcolare, sopratutto quelle legate e derivanti dalla matrice di confusione. L'accesso a queste metriche puo' essere fatto singolarmente con appositi metodi:\n",
    "\n",
    "Rif: [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "E' pero' piu' rapido, avendo una matrice di confusione calcolata, chiedere un report complessivo che mostri informazioni sulle singole etichette/classi predette e sull'intera matrice.\n",
    "\n",
    "Per farlo, il metodo _classification_report_ richiedera' solamente etichette reali, predizioni ed eventualmente i nomi da associare alle etichette numeriche.\n",
    "\n",
    "Rif: [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(real_labels, pred_labels, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dal report si ha un riassunto delle principali metriche per classe: recall, precision, f1-score...per ognuna delle classi di dati.\n",
    "* Dal report si hanno informazioni complessive, prima fra tutte l'accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _La curva ROC e la curva Precision-Recall._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Curva ROC._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso di classificazioni **binarie**, _scikit_ fornisce due metodi per il calcolo delle curve ROC e Precision-Recall.\n",
    "Predisponiamo quindi i dati simulati per una classificazione binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Malato', 'Sano']\n",
    "\n",
    "num_data = 100\n",
    "\n",
    "real_labels = np.random.randint(0, len(classes), size=(num_data))\n",
    "pred_labels = np.random.randint(0, len(classes), size=(num_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date due classi, abbiamo un vettore di etichette e un vettore di previsioni fatte. Vi aggiungiamo quindi un vettore di 'confidenze' sul fatto che una etichetta predetta appartenza alla classe positiva. La classe positiva sara' per convenzione la 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_labels = np.random.rand(num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo quindi i vettori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per utilizzare il metodo _roc_curve_, sara' necessario fornire le previsioni attese e la confidenza sul fatto che queste siano positive o meno. Per specificare l'etichetta positiva, si utilizza il parametro _pos_label_.\n",
    "\n",
    "* Rif: [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(real_labels, conf_labels, pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il calcolo della curva ROC, si calcolano diversi 'valori di soglia', _thresholds_.\n",
    "Questi valori sono utilizzati come discriminanti per dire quali fra le 'confidenze' saranno da associare alla classe positiva e quali alla negativa.\n",
    "Di seguito le soglie utilizzate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le soglie, confrontate di volta in volta con le confidenze, permettono di calcolare ad ogni passo il valore di TPR ed FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, values in enumerate(zip(thresholds, tpr, fpr)):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    else:\n",
    "        print(f'Soglia {values[0]:.5f} --> Tpf: {values[1]:.5f} - Fpr: {values[2]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficando questi risultati, si ha una stima di quanto e' buono il modello di classificazione dal valore di area sotteso dalla curva. Nel caso ottimo, pari ad 1.\n",
    "Il calcolo, lo fornisce il metodo _auc_ che richiede solamente di avere la liste dei valori di TPR ed FPR.\n",
    "\n",
    "* Rif: [auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafichiamo il tutto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Curva Precision-Recall._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto visto per la curva ROC, si applica per la curva Precision-Recall. Il metodo in questo caso e' _precision_recall_curve_.\n",
    "\n",
    "* Rif: [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(real_labels, conf_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso viene fatta muovere una soglia e con questa si calcola il variare di precision e recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito, alcuni dei valori ottenuti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, values in enumerate(zip(thresholds, precision, recall)):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    else:\n",
    "        print(f'Soglia {values[0]:.5f} --> Precision: {values[1]:.5f} - Recall: {values[2]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, il calcolo dell'area sottesa indichera' la bonta' del modello. L'ottimo si raggiunge in 1.\n",
    "Per il calcolo e' possibile utilizzare due metodi: _auc_ e _average_precision_score_.\n",
    "\n",
    "* Rif: [average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    "\n",
    "All'aumentare dei punti della curva, i risultati tendono ad assomigliarsi sempre piu'. La principale differenza nel calcolo e' il modo in cui i punti del grafico vengono interpolati per eseguire il calcolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc_1 = auc(recall, precision)\n",
    "pr_auc_2 = average_precision_score(real_labels, conf_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafichiamo il tutto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (area = {pr_auc_1:0.3f} | {pr_auc_2:0.3f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CorsoAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
